#!/bin/bash

# Start with a fresh install
if [ $(ps -ef | grep 'perl /root' | wc -l) -eq 2 ] # if Druid is running, kill it
then
  kill $(ps -ef | grep 'perl /root' | awk 'NF{print $2}' | head -n 1)
  while [ $(curl localhost:8888/ 2>&1 >/dev/null | grep Fail | wc -w) -eq 0 ]; do sleep 1; done
fi
# remove the installation (if it exists) so we can start fresh
rm -rf /root/apache-druid-24.0.0/
# Make sure we have the tar file
if [ ! -f /root/apache-druid-24.0.0-bin.tar.gz ]
then
  wget https://ftp.wayne.edu/apache/druid/24.0.0/apache-druid-24.0.0-bin.tar.gz
fi
tar -xzf apache-druid-24.0.0-bin.tar.gz

# Install the tools for this lab
apt-get update
if [ $(dpkg -l | grep 'ii  bsdmainutils' | wc -l) -eq 0 ]
then
  apt-get -y install bsdmainutils
fi

if [ $(dpkg -l | grep 'ii  tree' | wc -l) -eq 0 ]
then
  apt install tree
fi

if [ $(dpkg -l | grep 'ii  stunnel4' | wc -l) -eq 0 ]
then
  apt-get -y install stunnel4
fi

if [ $(dpkg -l | grep 'ii  less' | wc -l) -eq 0 ]
then
  apt-get -y install less
fi

if [ $(dpkg -l | grep 'ii  multitail' | wc -l) -eq 0 ]
then
  apt-get -y install multitail
fi


# Install MinIO
if [ ! -f /root/apache-druid-24.0.0/minio ]
then
  wget https://dl.minio.io/server/minio/release/linux-amd64/minio
  chmod +x minio
fi
if [ $(ps -ef | grep './minio server' | wc -l) -lt 2 ] # if Druid is running, kill it
then
  nohup ./minio server /minio --console-address :9090 &
fi

if [ ! -f /usr/local/bin/mc ]
then
wget https://dl.minio.io/client/mc/release/linux-amd64/mc
chmod +x mc
mv mc /usr/local/bin
fi

mc alias set local http://localhost:9000 minioadmin minioadmin

if [ $(mc admin user svcacct ls local minioadmin | grep access123 | wc -w) -lt 1 ]
then
  mc admin user svcacct add local minioadmin --access-key access123   --secret-key secret1234567890
fi

if [ $(mc ls local | grep druidlocal/ | awk 'NF{print $5}' | wc -w) -lt 1 ]
then
  mc mb local/druidlocal
fi


cat > /root/apache-druid-24.0.0/conf/druid/single-server/nano-quickstart/_common/common.runtime.properties << \EOF
#
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
#

# Extensions specified in the load list will be loaded by Druid
# We are using local fs for deep storage - not recommended for production - use S3, HDFS, or NFS instead
# We are using local derby for the metadata store - not recommended for production - use MySQL or Postgres instead

# If you specify `druid.extensions.loadList=[]`, Druid won't load any extension from file system.
# If you don't specify `druid.extensions.loadList`, Druid will load all the extensions under root extension directory.
# More info: https://druid.apache.org/docs/latest/operations/including-extensions.html
druid.extensions.loadList=["druid-hdfs-storage", "druid-kafka-indexing-service", "druid-datasketches", "druid-multi-stage-query", "druid-s3-extensions"]

# If you have a different version of Hadoop, place your Hadoop client jar files in your hadoop-dependencies directory
# and uncomment the line below to point to your directory.
#druid.extensions.hadoopDependenciesDir=/my/dir/hadoop-dependencies


#
# Hostname
#
druid.host=localhost

#
# Logging
#

# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.startup.logging.logProperties=true

#
# Zookeeper
#

druid.zk.service.host=localhost
druid.zk.paths.base=/druid

#
# Metadata storage
#

# For Derby server on your Druid Coordinator (only viable in a cluster with a single Coordinator, no fail-over):
druid.metadata.storage.type=derby
druid.metadata.storage.connector.connectURI=jdbc:derby://localhost:1527/var/druid/metadata.db;create=true
druid.metadata.storage.connector.host=localhost
druid.metadata.storage.connector.port=1527

# For MySQL (make sure to include the MySQL JDBC driver on the classpath):
#druid.metadata.storage.type=mysql
#druid.metadata.storage.connector.connectURI=jdbc:mysql://db.example.com:3306/druid
#druid.metadata.storage.connector.user=...
#druid.metadata.storage.connector.password=...

# For PostgreSQL:
#druid.metadata.storage.type=postgresql
#druid.metadata.storage.connector.connectURI=jdbc:postgresql://db.example.com:5432/druid
#druid.metadata.storage.connector.user=...
#druid.metadata.storage.connector.password=...

#
# Deep storage
#

druid.storage.type: s3
druid.storage.bucket: druidlocal
druid.storage.baseKey: druid/segments
druid.s3.accessKey: access123
druid.s3.secretKey: secret1234567890
druid.s3.forceGlobalBucketAccessEnabled: false
druid.storage.disableAcl: true
druid.s3.enablePathStyleAccess: true
druid.s3.endpoint.signingRegion: us-east-1
druid.s3.endpoint.url: http://localhost:9000
druid.s3.protocol: http
druid.s3.enablePathStyleAccess: true

#
# Indexing service logs
#

druid.indexer.logs.type: s3
druid.indexer.logs.s3Bucket: druidlocal
druid.indexer.logs.s3Prefix: druid/logs
druid.indexer.logs.disableAcl: true

#
# Service discovery
#

druid.selectors.indexing.serviceName=druid/overlord
druid.selectors.coordinator.serviceName=druid/coordinator

#
# Monitoring
#

druid.monitoring.monitors=["org.apache.druid.java.util.metrics.JvmMonitor"]
druid.emitter=noop
druid.emitter.logging.logLevel=info

# Storage type of double columns
# ommiting this will lead to index double as float at the storage layer

druid.indexing.doubleStorage=double

#
# Security
#
druid.server.hiddenProperties=["druid.s3.accessKey","druid.s3.secretKey","druid.metadata.storage.connector.password", "password", "key", "token", "pwd"]


#
# SQL
#
druid.sql.enable=true

# Planning SQL query when there is aggregate distinct in the statement
druid.sql.planner.useGroupingSetForExactDistinct=true

#
# Lookups
#
druid.lookup.enableLookupSyncOnStartup=false

#
# Expression processing config
#
druid.expressions.useStrictBooleans=true

#
# Http client
#
druid.global.http.eagerInitialization=false
EOF

# set up stunnel so we can run the Druid console and MinIO in tabs
cat > /root/webserver.conf << \EOF
pid = /root/stunnel.pid

[https]
accept = 8443
connect = 8888
cert = keyfile.pem
EOF


if [ ! -f /root/keyfile.pem ]
then
  cat > /root/keyfile.pem << \EOF
-----BEGIN PRIVATE KEY-----
MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDRo5VkCKXJOEBn
rMwg8ngUDr7VQ3HOcZlU/6hpjDsXjdPC+o9QvwF47zgEUAlZ0S82Ebo9ja8/V7fX
tCMiXH8XP8xmI41wkeECZQ4cFPeB4RAT3q893958Wsapv2e7syns957/1hSdf+rQ
/iOFwXFelFybuD4FWUCwaoSxajs4LVBYWGv+sL2Cl8UWJ3wokiiwfPQqHC+4SBb2
ExgH664bQZ3tSdSOzegNUWYoe6OG+9cjrb+X8OFTrYBHFDXa7irMSXWngLs1RVan
r6sP+3Glxq+GoEdQoIOi0hyZADyf+ZeUDA3sBkcrvVCpFWyfJLng4UvKdk02yNtB
43TyGv9NAgMBAAECggEAeF7TInMMrjMMV/sR4kEWzX2B+XYWXJFBl1qMWr5YF7gp
xSSVManasJO6uE80b0v7LbKUG0/FNjuppcMbAVnW54SMWoFDb0Vzyyo3JcysKPkP
gDk2qfJnD0QYGdN7Pn0rL5MmyoMjpyORspqZzu05qTLK0ebCRdPvntwqgKwcgK4J
+p1S8a+fhAPyNe/5oAmEro9iuiq6fea1lDpdl3AmOvi4sjkp8hpsw1k/4ryZZD45
Xrvbw3JtH36UnyS9o//fWY0UsvvOrd/ULIJAwUZQtWelEXxiYmqnQ8J0bCcXTpq7
chIZGFk3zUN3V/FuqZp7r+O4k5QPUnFYGu4HHzRCyQKBgQDtqQ1c1PWJ+bKAai3B
dnRd+64BuhuH3Sf5oXmPSZElHO8p8pPcsoLTOKZf+7VeFky0Dd3bWGyMBi1djNau
soT6cPCNmYLNYJOet58DIerld2q801F76s2aoJ4Vr8Fe8frUu0T7DlojIoXtWkxJ
UKM92rj+agzNzkVFlpIffO3LfwKBgQDh0Pkjtw38lPfI4jjWcEuquzPsWFFj442X
43AJXS4gehPJCeVwYBSASzRznABo1nJ/O7XjBz2TyGS368VoS0XwkzYWKl709m8d
sirvOr3gazQbIVg1+6Xcwp+fkXLDdnDgLRybk97WGLt8bcL72vfSdiPkA+ZCldTk
MEM8y4MLMwKBgQCV98Y53Rwv+8Pa6qWC3geiQq3T4yd6PWD7UhzOAUM44yA8HRmW
+4I4XRgHm2MgQvwV5mYoAo1EcqVv0e6qgHYPPvNctxDmHloonoAVzvJV6FwPWV2i
z1D8kPwQfT6Ndi6szJtDvRO99gwC8acP4HWSE5IG4M7Bk7b8qX39pqy7awKBgBNa
ps/R06gPWIFmKjru2v1iGBwsoaUvDqay/po+20I7VFDJwVl1UqyARZmXCE3z+1I+
WuN/bl6pMUC2O3Ap62+oHl2d+g+Cw5lG+PkSDs4aQhgFLptJ2zhlkerKO4zXLOt5
0lbfQJ0W95K9kZWmedEXARJ85xJT5zA7lreyNaQTAoGBAOVV+UPbtX4fodKW3wUF
9Wu2SWDl7dMugT/SIxBELXowg5wbUEDqY1egevNkzITfRuRK8dUmQN2BXHB6o8rt
OWYPAArAlIPhfMZ48GiPqvJix5v4RcL+dXrLpcIBerym8R3qii0vHM7FHOYux1PD
cVpCyhG+KgfJRfbhyhiT3F7m
-----END PRIVATE KEY-----
-----BEGIN CERTIFICATE-----
MIIC2TCCAcGgAwIBAgIUB1uIZhHQIL9DQyuVeGjmQD/u30IwDQYJKoZIhvcNAQEL
BQAwFDESMBAGA1UEAwwJbG9jYWxob3N0MB4XDTIyMDMyOTIyMTMwNFoXDTMyMDMy
NjIyMTMwNFowFDESMBAGA1UEAwwJbG9jYWxob3N0MIIBIjANBgkqhkiG9w0BAQEF
AAOCAQ8AMIIBCgKCAQEA0aOVZAilyThAZ6zMIPJ4FA6+1UNxznGZVP+oaYw7F43T
wvqPUL8BeO84BFAJWdEvNhG6PY2vP1e317QjIlx/Fz/MZiONcJHhAmUOHBT3geEQ
E96vPd/efFrGqb9nu7Mp7Pee/9YUnX/q0P4jhcFxXpRcm7g+BVlAsGqEsWo7OC1Q
WFhr/rC9gpfFFid8KJIosHz0KhwvuEgW9hMYB+uuG0Gd7UnUjs3oDVFmKHujhvvX
I62/l/DhU62ARxQ12u4qzEl1p4C7NUVWp6+rD/txpcavhqBHUKCDotIcmQA8n/mX
lAwN7AZHK71QqRVsnyS54OFLynZNNsjbQeN08hr/TQIDAQABoyMwITAJBgNVHRME
AjAAMBQGA1UdEQQNMAuCCWxvY2FsaG9zdDANBgkqhkiG9w0BAQsFAAOCAQEAPKD5
7cv0wk1v45ui6M9ui+jyNiC4cU6J5CfIhxbBMmkEv31yPxCTGNyk04ic5EoW6gKt
kI0A7uNPSd7ph5gWwMpCNfA7uUs8XHARJcAxasGeu6JVYILI52rhDGznjetisPWM
z/BEDhHYDEWsbgkpy46/a/ha9gddZ5apA4s9JAm3fiQ9g1V5sEIC2lJpyaYCIZdN
mAbCQrJGc7MqUgOQKnLpw2inOr3K+tEJPG7PmY2yJ0geSA3lrnbjIdM8YQQ+jSWJ
Nx20QuT2mjFoq+vJg0o9acJvkZQjnIHuKkUAzK7aH1fTRx77d/5SGBepLOP6GTyj
kCztd6hQYUZe3MbxuA==
-----END CERTIFICATE-----
EOF

  chmod +r keyfile.pem
fi

# if stunnel is running, kill it
if [ $(ps -ef | grep 'stunnel' | wc -l) -eq 2 ]
then
  kill $(ps -ef | grep 'stunnel' | awk 'NF{print $2}' | head -n 1)
fi
# (re)start stunnel
nohup stunnel webserver.conf > /dev/null 2> /dev/null < /dev/null & disown

cat > /root/ingestion.json << \EOF
{
    "query":"INSERT INTO wikipedia SELECT TIME_PARSE(\"timestamp\") AS __time, page FROM TABLE( EXTERN( '{\"type\": \"http\", \"uris\": [\"https://static.imply.io/gianm/wikipedia-2016-06-27-sampled.json\"]}', '{\"type\": \"json\"}', '[{\"name\": \"timestamp\", \"type\": \"string\"}, {\"name\": \"page\", \"type\": \"string\"}, {\"name\": \"user\", \"type\": \"string\"}]')) PARTITIONED BY DAY"
}
EOF
